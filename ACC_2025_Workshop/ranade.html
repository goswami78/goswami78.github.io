<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Decomposing prediction mechanisms for in-context recall: A toy problem using dynamical systems</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="program.html">Program</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Decomposing prediction mechanisms for in-context recall: A toy problem using dynamical systems</h1>
</div>
<h3>Speaker</h3>
<p>Gireeja Ranade
</p>
<h3>Affiliation</h3>
<p>Associate Teaching Professor<br />
Department of Electrical Engineering and Computer Science<br />
University of California, Berkeley
</p>
<h3>Abstract</h3>
<p>We introduce a new family of dynamical system-based toy problems to study in-context learning (ICL) and associative recall in transformer models. These problems combine features of linear-regression-style continuous in-context learning (ICL) with discrete associative recall. Specifically, we consider symbolically-labeled interleaved observation segments from randomly drawn linear deterministic dynamical systems. We pretrain the transformer models on sample traces from this toy, and study if it can recall the state of a sequence previously seen in its context when prompted to do so with its in-context label. Performing the task requires the transformer model to do multiple tasks &#8201;&mdash;&#8201; e.g. identifying a dynamical system that a particular segment belongs to, or continuing a prediction once the prediction has been initiated etc. We find that the ability to predict the first token in a segment (which is similar to identifying the underlying system) shows emergence well into training. Surprisingly, predicting the second token in the segment (which is similar to continuing a prediction) can actually be done earlier, before the ability to predict the first token emerges.
</p>
<p>Via out-of-distribution experiments, and a mechanistic analysis on model weights via edge pruning, we find that next-token prediction for this toy problem involves at least two separate mechanisms. One mechanism uses the discrete labels to do the associative recall required to predict the start of a resumption of a previously seen sequence. The second mechanism, which is largely agnostic to the discrete labels, performs a Bayesian-style prediction based on the previous token and the context. These two mechanisms have different learning dynamics. To confirm that this multi-mechanism (manifesting as separate phase transitions) phenomenon is not just an artifact of our toy setting, we used OLMo training checkpoints on an ICL translation task to see a similar phenomenon: a decisive gap in the emergence of first-task-token performance vs second-task-token performance.
</p>
<h3>Bio</h3>
<table class="imgtable"><tr><td>
<img src="photos/gireeja.png" alt="alt text" width="200px" height="300px" />&nbsp;</td>
<td align="left"><p>Gireeja Ranade is an Associate Teaching Professor in Electrical Engineering and Computer Sciences (EECS) at the University of California, Berkeley. Before this she was at Microsoft Research in Redmond, WA. She received her PhD in Electrical Engineering and Computer Science from the University of California, Berkeley, and her undergraduate degree from MIT in Cambridge, MA. She is the recipient of some awards for her research and teaching such as the NSF CAREER Award, the UC Berkeley Electrical Engineering Award for Outstanding Teaching, the UC Berkeley Award for Extraordinary Teaching in Extraordinary Times and others. 
</p>
</td></tr></table>
</td>
</tr>
</table>
</body>
</html>
